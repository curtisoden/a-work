{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aff49171",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Cleaning and Merging Workflow\n",
      "==================================================\n",
      "\n",
      "1. Loading Source Datasets\n",
      "------------------------------\n",
      "Client data shape: (14606, 26)\n",
      "Client data columns: ['id', 'channel_sales', 'cons_12m', 'cons_gas_12m', 'cons_last_month', 'date_activ', 'date_end', 'date_modif_prod', 'date_renewal', 'forecast_cons_12m', 'forecast_cons_year', 'forecast_discount_energy', 'forecast_meter_rent_12m', 'forecast_price_energy_off_peak', 'forecast_price_energy_peak', 'forecast_price_pow_off_peak', 'has_gas', 'imp_cons', 'margin_gross_pow_ele', 'margin_net_pow_ele', 'nb_prod_act', 'net_margin', 'num_years_antig', 'origin_up', 'pow_max', 'churn']\n",
      "Price data shape: (193002, 8)\n",
      "Price data columns: ['id', 'price_date', 'price_off_peak_var', 'price_peak_var', 'price_mid_peak_var', 'price_off_peak_fix', 'price_peak_fix', 'price_mid_peak_fix']\n",
      "\n",
      "Client data sample:\n",
      "                                 id                     channel_sales  \\\n",
      "0  24011ae4ebbe3035111d65fa7c15bc57  foosdfpfkusacimwkcsosbicdxkicaua   \n",
      "1  d29c2c54acc38ff3c0614d0a653813dd                           MISSING   \n",
      "\n",
      "   cons_12m  cons_gas_12m  cons_last_month  date_activ    date_end  \\\n",
      "0         0         54946                0  2013-06-15  2016-06-15   \n",
      "1      4660             0                0  2009-08-21  2016-08-30   \n",
      "\n",
      "  date_modif_prod date_renewal  forecast_cons_12m  ...  has_gas  imp_cons  \\\n",
      "0      2015-11-01   2015-06-23               0.00  ...        t       0.0   \n",
      "1      2009-08-21   2015-08-31             189.95  ...        f       0.0   \n",
      "\n",
      "   margin_gross_pow_ele  margin_net_pow_ele  nb_prod_act  net_margin  \\\n",
      "0                 25.44               25.44            2      678.99   \n",
      "1                 16.38               16.38            1       18.89   \n",
      "\n",
      "  num_years_antig                         origin_up  pow_max  churn  \n",
      "0               3  lxidpiddsbxsbosboudacockeimpuepw   43.648      1  \n",
      "1               6  kamkkxfxxuwbdslkwifmmcsiusiuosws   13.800      0  \n",
      "\n",
      "[2 rows x 26 columns]\n",
      "\n",
      "Price data sample:\n",
      "                                 id  price_date  price_off_peak_var  \\\n",
      "0  038af19179925da21a25619c5a24b745  2015-01-01            0.151367   \n",
      "1  038af19179925da21a25619c5a24b745  2015-02-01            0.151367   \n",
      "\n",
      "   price_peak_var  price_mid_peak_var  price_off_peak_fix  price_peak_fix  \\\n",
      "0             0.0                 0.0           44.266931             0.0   \n",
      "1             0.0                 0.0           44.266931             0.0   \n",
      "\n",
      "   price_mid_peak_fix  \n",
      "0                 0.0  \n",
      "1                 0.0  \n",
      "\n",
      "Client data types:\n",
      "id                                 object\n",
      "channel_sales                      object\n",
      "cons_12m                            int64\n",
      "cons_gas_12m                        int64\n",
      "cons_last_month                     int64\n",
      "date_activ                         object\n",
      "date_end                           object\n",
      "date_modif_prod                    object\n",
      "date_renewal                       object\n",
      "forecast_cons_12m                 float64\n",
      "forecast_cons_year                  int64\n",
      "forecast_discount_energy          float64\n",
      "forecast_meter_rent_12m           float64\n",
      "forecast_price_energy_off_peak    float64\n",
      "forecast_price_energy_peak        float64\n",
      "forecast_price_pow_off_peak       float64\n",
      "has_gas                            object\n",
      "imp_cons                          float64\n",
      "margin_gross_pow_ele              float64\n",
      "margin_net_pow_ele                float64\n",
      "nb_prod_act                         int64\n",
      "net_margin                        float64\n",
      "num_years_antig                     int64\n",
      "origin_up                          object\n",
      "pow_max                           float64\n",
      "churn                               int64\n",
      "dtype: object\n",
      "\n",
      "Price data types:\n",
      "id                     object\n",
      "price_date             object\n",
      "price_off_peak_var    float64\n",
      "price_peak_var        float64\n",
      "price_mid_peak_var    float64\n",
      "price_off_peak_fix    float64\n",
      "price_peak_fix        float64\n",
      "price_mid_peak_fix    float64\n",
      "dtype: object\n",
      "\n",
      "2. Data Exploration and Understanding\n",
      "----------------------------------------\n",
      "Client data info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 14606 entries, 0 to 14605\n",
      "Data columns (total 26 columns):\n",
      " #   Column                          Non-Null Count  Dtype  \n",
      "---  ------                          --------------  -----  \n",
      " 0   id                              14606 non-null  object \n",
      " 1   channel_sales                   14606 non-null  object \n",
      " 2   cons_12m                        14606 non-null  int64  \n",
      " 3   cons_gas_12m                    14606 non-null  int64  \n",
      " 4   cons_last_month                 14606 non-null  int64  \n",
      " 5   date_activ                      14606 non-null  object \n",
      " 6   date_end                        14606 non-null  object \n",
      " 7   date_modif_prod                 14606 non-null  object \n",
      " 8   date_renewal                    14606 non-null  object \n",
      " 9   forecast_cons_12m               14606 non-null  float64\n",
      " 10  forecast_cons_year              14606 non-null  int64  \n",
      " 11  forecast_discount_energy        14606 non-null  float64\n",
      " 12  forecast_meter_rent_12m         14606 non-null  float64\n",
      " 13  forecast_price_energy_off_peak  14606 non-null  float64\n",
      " 14  forecast_price_energy_peak      14606 non-null  float64\n",
      " 15  forecast_price_pow_off_peak     14606 non-null  float64\n",
      " 16  has_gas                         14606 non-null  object \n",
      " 17  imp_cons                        14606 non-null  float64\n",
      " 18  margin_gross_pow_ele            14606 non-null  float64\n",
      " 19  margin_net_pow_ele              14606 non-null  float64\n",
      " 20  nb_prod_act                     14606 non-null  int64  \n",
      " 21  net_margin                      14606 non-null  float64\n",
      " 22  num_years_antig                 14606 non-null  int64  \n",
      " 23  origin_up                       14606 non-null  object \n",
      " 24  pow_max                         14606 non-null  float64\n",
      " 25  churn                           14606 non-null  int64  \n",
      "dtypes: float64(11), int64(7), object(8)\n",
      "memory usage: 2.9+ MB\n",
      "None\n",
      "\n",
      "Price data info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 193002 entries, 0 to 193001\n",
      "Data columns (total 8 columns):\n",
      " #   Column              Non-Null Count   Dtype  \n",
      "---  ------              --------------   -----  \n",
      " 0   id                  193002 non-null  object \n",
      " 1   price_date          193002 non-null  object \n",
      " 2   price_off_peak_var  193002 non-null  float64\n",
      " 3   price_peak_var      193002 non-null  float64\n",
      " 4   price_mid_peak_var  193002 non-null  float64\n",
      " 5   price_off_peak_fix  193002 non-null  float64\n",
      " 6   price_peak_fix      193002 non-null  float64\n",
      " 7   price_mid_peak_fix  193002 non-null  float64\n",
      "dtypes: float64(6), object(2)\n",
      "memory usage: 11.8+ MB\n",
      "None\n",
      "\n",
      "Unique client IDs: 14606\n",
      "Total client records: 14606\n",
      "Unique price IDs: 16096\n",
      "Total price records: 193002\n",
      "\n",
      "Checking for target variable:\n",
      "âœ“ Found 'churn' column in client data\n",
      "Churn distribution: churn\n",
      "0    13187\n",
      "1     1419\n",
      "Name: count, dtype: int64\n",
      "Churn rate: 0.097\n",
      "\n",
      "3. Date Column Processing\n",
      "------------------------------\n",
      "Date columns in client data: ['date_activ', 'date_end', 'date_modif_prod', 'date_renewal']\n",
      "Date columns in price data: ['price_date']\n",
      "\n",
      "Converting date_activ:\n",
      "Date range: 2003-05-09 00:00:00 to 2014-09-01 00:00:00\n",
      "Epoch range: 1052438400 to 1409529600\n",
      "Normalized range: 0.000 to 1.000\n",
      "\n",
      "Converting date_end:\n",
      "Date range: 2016-01-28 00:00:00 to 2017-06-13 00:00:00\n",
      "Epoch range: 1453939200 to 1497312000\n",
      "Normalized range: 0.000 to 1.000\n",
      "\n",
      "Converting date_modif_prod:\n",
      "Date range: 2003-05-09 00:00:00 to 2016-01-29 00:00:00\n",
      "Epoch range: 1052438400 to 1454025600\n",
      "Normalized range: 0.000 to 1.000\n",
      "\n",
      "Converting date_renewal:\n",
      "Date range: 2013-06-26 00:00:00 to 2016-01-28 00:00:00\n",
      "Epoch range: 1372204800 to 1453939200\n",
      "Normalized range: 0.000 to 1.000\n",
      "\n",
      "Converting price_date:\n",
      "Date range: 2015-01-01 00:00:00 to 2015-12-01 00:00:00\n",
      "Epoch range: 1420070400 to 1448928000\n",
      "Normalized range: 0.000 to 1.000\n",
      "\n",
      "4. Merging Client and Price Data\n",
      "-----------------------------------\n",
      "Performing left join to keep all clients...\n",
      "Merged dataset shape: (175149, 34)\n",
      "Clients without price data: 0\n",
      "\n",
      "5. Creating Price Statistical Features\n",
      "----------------------------------------\n",
      "Found price columns: ['price_off_peak_var', 'price_peak_var', 'price_mid_peak_var', 'price_off_peak_fix', 'price_peak_fix', 'price_mid_peak_fix', 'price_date_epoch']\n",
      "Calculating price statistics per client...\n",
      "Price statistics shape: (16096, 36)\n",
      "Sample price statistics:\n",
      "                                 id  price_off_peak_var_mean  \\\n",
      "0  0002203ffbb812588b632b9e628cc38d                 0.124338   \n",
      "1  0004351ebdd665e6ee664792efc4fd13                 0.146426   \n",
      "\n",
      "   price_off_peak_var_std  price_off_peak_var_min  price_off_peak_var_max  \\\n",
      "0                0.003976                0.119906                0.128067   \n",
      "1                0.002197                0.143943                0.148405   \n",
      "\n",
      "   price_off_peak_var_last  price_peak_var_mean  price_peak_var_std  \\\n",
      "0                 0.119906             0.103794            0.001989   \n",
      "1                 0.143943             0.000000            0.000000   \n",
      "\n",
      "   price_peak_var_min  price_peak_var_max  ...  price_mid_peak_fix_mean  \\\n",
      "0            0.101673            0.105842  ...                16.280694   \n",
      "1            0.000000            0.000000  ...                 0.000000   \n",
      "\n",
      "   price_mid_peak_fix_std  price_mid_peak_fix_min  price_mid_peak_fix_max  \\\n",
      "0                0.025366               16.226389               16.291555   \n",
      "1                0.000000                0.000000                0.000000   \n",
      "\n",
      "   price_mid_peak_fix_last  price_date_epoch_mean  price_date_epoch_std  \\\n",
      "0                16.291555               0.498503              0.328389   \n",
      "1                 0.000000               0.498503              0.328389   \n",
      "\n",
      "   price_date_epoch_min  price_date_epoch_max  price_date_epoch_last  \n",
      "0                   0.0                   1.0                    1.0  \n",
      "1                   0.0                   1.0                    1.0  \n",
      "\n",
      "[2 rows x 36 columns]\n",
      "Dataset with price features shape: (14606, 61)\n",
      "\n",
      "6. One-Hot Encoding Categorical Variables\n",
      "------------------------------------------\n",
      "Found categorical columns: ['id', 'channel_sales', 'has_gas', 'origin_up']\n",
      "\n",
      "Unique channel_sales values:\n",
      "channel_sales\n",
      "foosdfpfkusacimwkcsosbicdxkicaua    6754\n",
      "MISSING                             3725\n",
      "lmkebamcaaclubfxadlmueccxoimlema    1843\n",
      "usilxuppasemubllopkaafesmlibmsdf    1375\n",
      "ewpakwlliwisiwduibdlfmalxowmwpci     893\n",
      "sddiedcslfslkckwlfkdpoeeailfpeds      11\n",
      "epumfxlbckeskwekxbiuasklxalciiuu       3\n",
      "fixdbufsefwooaasfcxdxadsiekoceaa       2\n",
      "Name: count, dtype: int64\n",
      "Created 8 dummy variables for channel_sales:\n",
      "['channel_sales_MISSING', 'channel_sales_epumfxlbckeskwekxbiuasklxalciiuu', 'channel_sales_ewpakwlliwisiwduibdlfmalxowmwpci', 'channel_sales_fixdbufsefwooaasfcxdxadsiekoceaa', 'channel_sales_foosdfpfkusacimwkcsosbicdxkicaua', 'channel_sales_lmkebamcaaclubfxadlmueccxoimlema', 'channel_sales_sddiedcslfslkckwlfkdpoeeailfpeds', 'channel_sales_usilxuppasemubllopkaafesmlibmsdf']\n",
      "\n",
      "Unique has_gas values:\n",
      "has_gas\n",
      "f    11955\n",
      "t     2651\n",
      "Name: count, dtype: int64\n",
      "Created 2 dummy variables for has_gas:\n",
      "['has_gas_f', 'has_gas_t']\n",
      "\n",
      "Unique origin_up values:\n",
      "origin_up\n",
      "lxidpiddsbxsbosboudacockeimpuepw    7097\n",
      "kamkkxfxxuwbdslkwifmmcsiusiuosws    4294\n",
      "ldkssxwpmemidmecebumciepifcamkci    3148\n",
      "MISSING                               64\n",
      "usapbepcfoloekilkwsdiboslwaxobdp       2\n",
      "ewxeelcelemmiwuafmddpobolfuxioce       1\n",
      "Name: count, dtype: int64\n",
      "Created 6 dummy variables for origin_up:\n",
      "['origin_up_MISSING', 'origin_up_ewxeelcelemmiwuafmddpobolfuxioce', 'origin_up_kamkkxfxxuwbdslkwifmmcsiusiuosws', 'origin_up_ldkssxwpmemidmecebumciepifcamkci', 'origin_up_lxidpiddsbxsbosboudacockeimpuepw', 'origin_up_usapbepcfoloekilkwsdiboslwaxobdp']\n",
      "\n",
      "Dataset shape after one-hot encoding: (14606, 77)\n",
      "\n",
      "7. Creating Consumption Features\n",
      "-----------------------------------\n",
      "Found consumption columns: ['cons_12m', 'cons_gas_12m', 'cons_last_month', 'forecast_cons_12m', 'forecast_cons_year', 'imp_cons']\n",
      "Created consumption difference features\n",
      "\n",
      "8. Creating Price Change Features\n",
      "-----------------------------------\n",
      "Created price_off_peak_var_dif and price_off_peak_var_perc\n",
      "Created price_peak_var_dif and price_peak_var_perc\n",
      "Created price_mid_peak_var_dif and price_mid_peak_var_perc\n",
      "Created price_off_peak_fix_dif and price_off_peak_fix_perc\n",
      "Created price_peak_fix_dif and price_peak_fix_perc\n",
      "Created price_mid_peak_fix_dif and price_mid_peak_fix_perc\n",
      "Created price_date_epoch_dif and price_date_epoch_perc\n",
      "\n",
      "9. Handle Target Variable\n",
      "------------------------------\n",
      "Using existing churn column\n",
      "Churn distribution:\n",
      "churn\n",
      "0    13187\n",
      "1     1419\n",
      "Name: count, dtype: int64\n",
      "Churn rate: 0.097\n",
      "\n",
      "10. Data Cleaning and Normalization\n",
      "-----------------------------------\n",
      "Normalizing 72 numerical features...\n",
      "Completed normalization and missing value handling\n",
      "\n",
      "11. Remove Non-Numeric Columns\n",
      "-----------------------------------\n",
      "Converting boolean column 'channel_sales_MISSING' to int\n",
      "Converting boolean column 'channel_sales_epumfxlbckeskwekxbiuasklxalciiuu' to int\n",
      "Converting boolean column 'channel_sales_ewpakwlliwisiwduibdlfmalxowmwpci' to int\n",
      "Converting boolean column 'channel_sales_fixdbufsefwooaasfcxdxadsiekoceaa' to int\n",
      "Converting boolean column 'channel_sales_foosdfpfkusacimwkcsosbicdxkicaua' to int\n",
      "Converting boolean column 'channel_sales_lmkebamcaaclubfxadlmueccxoimlema' to int\n",
      "Converting boolean column 'channel_sales_sddiedcslfslkckwlfkdpoeeailfpeds' to int\n",
      "Converting boolean column 'channel_sales_usilxuppasemubllopkaafesmlibmsdf' to int\n",
      "Converting boolean column 'has_gas_f' to int\n",
      "Converting boolean column 'has_gas_t' to int\n",
      "Converting boolean column 'origin_up_MISSING' to int\n",
      "Converting boolean column 'origin_up_ewxeelcelemmiwuafmddpobolfuxioce' to int\n",
      "Converting boolean column 'origin_up_kamkkxfxxuwbdslkwifmmcsiusiuosws' to int\n",
      "Converting boolean column 'origin_up_ldkssxwpmemidmecebumciepifcamkci' to int\n",
      "Converting boolean column 'origin_up_lxidpiddsbxsbosboudacockeimpuepw' to int\n",
      "Converting boolean column 'origin_up_usapbepcfoloekilkwsdiboslwaxobdp' to int\n",
      "Found 3 non-numeric columns to remove:\n",
      "- channel_sales: object\n",
      "- has_gas: object\n",
      "- origin_up: object\n",
      "Dropped 3 non-numeric columns\n",
      "\n",
      "12. Final Dataset Preparation\n",
      "--------------------------------\n",
      "Final dataset shape: (14606, 78)\n",
      "Features: 77\n",
      "Target variable: churn\n",
      "\n",
      "13. Data Quality Checks\n",
      "-------------------------\n",
      "Data quality checks:\n",
      "- Missing values: 0\n",
      "- Infinite values: 0\n",
      "- Duplicate rows: 0\n",
      "- Data types: {dtype('float64'): 61, dtype('int32'): 17}\n",
      "âœ“ All columns are now numeric\n",
      "\n",
      "14. Saving Cleaned Dataset\n",
      "------------------------------\n",
      "Dataset saved as: DATA_v2_churn.csv\n",
      "Final shape: (14606, 78)\n",
      "\n",
      "15. Create Sample Dataset\n",
      "------------------------------\n",
      "Sample dataset saved as: SAMPLE_v2_churn.csv\n",
      "Sample shape: (150, 78)\n",
      "\n",
      "==================================================\n",
      "DATA CLEANING AND MERGING COMPLETE!\n",
      "==================================================\n",
      "\n",
      "Final column names:\n",
      "['cons_12m', 'cons_gas_12m', 'cons_last_month', 'forecast_cons_12m', 'forecast_cons_year', 'forecast_discount_energy', 'forecast_meter_rent_12m', 'forecast_price_energy_off_peak', 'forecast_price_energy_peak', 'forecast_price_pow_off_peak', 'imp_cons', 'margin_gross_pow_ele', 'margin_net_pow_ele', 'nb_prod_act', 'net_margin', 'num_years_antig', 'pow_max', 'price_off_peak_var_mean', 'price_off_peak_var_std', 'price_off_peak_var_min', 'price_off_peak_var_max', 'price_off_peak_var_last', 'price_peak_var_mean', 'price_peak_var_std', 'price_peak_var_min', 'price_peak_var_max', 'price_peak_var_last', 'price_mid_peak_var_mean', 'price_mid_peak_var_std', 'price_mid_peak_var_min', 'price_mid_peak_var_max', 'price_mid_peak_var_last', 'price_off_peak_fix_mean', 'price_off_peak_fix_std', 'price_off_peak_fix_min', 'price_off_peak_fix_max', 'price_off_peak_fix_last', 'price_peak_fix_mean', 'price_peak_fix_std', 'price_peak_fix_min', 'price_peak_fix_max', 'price_peak_fix_last', 'price_mid_peak_fix_mean', 'price_mid_peak_fix_std', 'price_mid_peak_fix_min', 'price_mid_peak_fix_max', 'price_mid_peak_fix_last', 'channel_sales_MISSING', 'channel_sales_epumfxlbckeskwekxbiuasklxalciiuu', 'channel_sales_ewpakwlliwisiwduibdlfmalxowmwpci', 'channel_sales_fixdbufsefwooaasfcxdxadsiekoceaa', 'channel_sales_foosdfpfkusacimwkcsosbicdxkicaua', 'channel_sales_lmkebamcaaclubfxadlmueccxoimlema', 'channel_sales_sddiedcslfslkckwlfkdpoeeailfpeds', 'channel_sales_usilxuppasemubllopkaafesmlibmsdf', 'has_gas_f', 'has_gas_t', 'origin_up_MISSING', 'origin_up_ewxeelcelemmiwuafmddpobolfuxioce', 'origin_up_kamkkxfxxuwbdslkwifmmcsiusiuosws', 'origin_up_ldkssxwpmemidmecebumciepifcamkci', 'origin_up_lxidpiddsbxsbosboudacockeimpuepw', 'origin_up_usapbepcfoloekilkwsdiboslwaxobdp', 'cons_pwr_12_mo_dif', 'cons_pwr_12_mo_perc', 'price_off_peak_var_dif', 'price_off_peak_var_perc', 'price_peak_var_dif', 'price_peak_var_perc', 'price_mid_peak_var_dif', 'price_mid_peak_var_perc', 'price_off_peak_fix_dif', 'price_off_peak_fix_perc', 'price_peak_fix_dif', 'price_peak_fix_perc', 'price_mid_peak_fix_dif', 'price_mid_peak_fix_perc', 'churn']\n",
      "\n",
      "First 3 rows of cleaned dataset:\n",
      "   cons_12m  cons_gas_12m  cons_last_month  forecast_cons_12m  \\\n",
      "0  0.000000      0.013225         0.000000           0.000000   \n",
      "1  0.000751      0.000000         0.000000           0.002291   \n",
      "2  0.000088      0.000000         0.000000           0.000579   \n",
      "3  0.000255      0.000000         0.000000           0.002895   \n",
      "4  0.000713      0.000000         0.000682           0.005377   \n",
      "5  0.001337      0.000000         0.002591           0.009613   \n",
      "6  0.007265      0.000000         0.000000           0.097334   \n",
      "7  0.004761      0.000000         0.001634           0.010431   \n",
      "8  0.000477      0.000000         0.000000           0.005360   \n",
      "9  0.004199      0.000000         0.002837           0.033028   \n",
      "\n",
      "   forecast_cons_year  forecast_discount_energy  forecast_meter_rent_12m  \\\n",
      "0            0.000000                       0.0                 0.002970   \n",
      "1            0.000000                       0.0                 0.027148   \n",
      "2            0.000000                       0.0                 0.064608   \n",
      "3            0.000000                       0.0                 0.033088   \n",
      "4            0.002999                       0.0                 0.219803   \n",
      "5            0.011393                       0.0                 0.050258   \n",
      "6            0.000000                       0.0                 0.000000   \n",
      "7            0.004282                       0.0                 0.241094   \n",
      "8            0.000000                       0.0                 0.026447   \n",
      "9            0.012476                       0.0                 0.217634   \n",
      "\n",
      "   forecast_price_energy_off_peak  forecast_price_energy_peak  \\\n",
      "0                        0.417870                    0.500788   \n",
      "1                        0.531864                    0.000000   \n",
      "2                        0.605169                    0.448521   \n",
      "3                        0.535452                    0.000000   \n",
      "4                        0.426700                    0.510346   \n",
      "5                        0.601450                    0.439500   \n",
      "6                        0.606571                    0.446679   \n",
      "7                        0.420400                    0.504335   \n",
      "8                        0.531864                    0.000000   \n",
      "9                        0.422542                    0.507305   \n",
      "\n",
      "   forecast_price_pow_off_peak  ...  price_peak_var_perc  \\\n",
      "0                     0.685156  ...             0.128062   \n",
      "1                     0.747665  ...             0.000000   \n",
      "2                     0.747665  ...             0.008635   \n",
      "3                     0.747665  ...             0.000000   \n",
      "4                     0.685156  ...             0.024290   \n",
      "5                     0.764487  ...             0.029609   \n",
      "6                     0.747665  ...             0.005291   \n",
      "7                     0.685156  ...             0.024576   \n",
      "8                     0.747665  ...             0.000000   \n",
      "9                     0.685156  ...             0.026210   \n",
      "\n",
      "   price_mid_peak_var_dif  price_mid_peak_var_perc  price_off_peak_fix_dif  \\\n",
      "0                0.647429                 0.000000            6.254426e-02   \n",
      "1                0.000000                 0.000000            3.004387e-03   \n",
      "2                0.000000                 0.000000            3.004370e-03   \n",
      "3                0.000000                 0.000000            3.004370e-03   \n",
      "4                0.031034                 0.009578            2.753193e-03   \n",
      "5                0.000000                 0.000000            1.689947e-08   \n",
      "6                0.000000                 0.000000            0.000000e+00   \n",
      "7                0.026590                 0.008057            2.753193e-03   \n",
      "8                0.000000                 0.000000            3.004387e-03   \n",
      "9                0.034329                 0.010447            0.000000e+00   \n",
      "\n",
      "   price_off_peak_fix_perc  price_peak_fix_dif  price_peak_fix_perc  \\\n",
      "0             4.647684e-02            0.669687             0.000000   \n",
      "1             2.045914e-03            0.000000             0.000000   \n",
      "2             2.045903e-03            0.000000             0.000000   \n",
      "3             2.045903e-03            0.000000             0.000000   \n",
      "4             2.045907e-03            0.002679             0.008142   \n",
      "5             1.150813e-08            0.000000             0.000000   \n",
      "6             0.000000e+00            0.000000             0.000000   \n",
      "7             2.045907e-03            0.002679             0.008142   \n",
      "8             2.045914e-03            0.000000             0.000000   \n",
      "9             0.000000e+00            0.000000             0.000000   \n",
      "\n",
      "   price_mid_peak_fix_dif  price_mid_peak_fix_perc  churn  \n",
      "0                0.960688                 0.000000      1  \n",
      "1                0.000000                 0.000000      0  \n",
      "2                0.000000                 0.000000      0  \n",
      "3                0.000000                 0.000000      0  \n",
      "4                0.003843                 0.004206      0  \n",
      "5                0.000000                 0.000000      1  \n",
      "6                0.000000                 0.000000      1  \n",
      "7                0.003843                 0.004206      0  \n",
      "8                0.000000                 0.000000      0  \n",
      "9                0.000000                 0.000000      0  \n",
      "\n",
      "[10 rows x 78 columns]\n",
      "\n",
      "Dataset ready for machine learning!\n",
      "Full dataset: DATA_v2_churn.csv\n",
      "Sample dataset: SAMPLE_v2_churn.csv\n"
     ]
    }
   ],
   "source": [
    "# Data Cleaning and Merging Workflow\n",
    "# This notebook merges SOURCE_client_data.csv and SOURCE_price_data.csv \n",
    "# to create a machine learning-ready dataset\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"Data Cleaning and Merging Workflow\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Load the source datasets\n",
    "print(\"\\n1. Loading Source Datasets\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# Load client data\n",
    "client_df = pd.read_csv('SOURCE_client_data.csv')\n",
    "print(f\"Client data shape: {client_df.shape}\")\n",
    "print(f\"Client data columns: {list(client_df.columns)}\")\n",
    "\n",
    "# Load price data  \n",
    "price_df = pd.read_csv('SOURCE_price_data.csv')\n",
    "print(f\"Price data shape: {price_df.shape}\")\n",
    "print(f\"Price data columns: {list(price_df.columns)}\")\n",
    "\n",
    "# Display first few rows to understand the data structure\n",
    "print(\"\\nClient data sample:\")\n",
    "print(client_df.head(2))\n",
    "print(\"\\nPrice data sample:\")\n",
    "print(price_df.head(2))\n",
    "\n",
    "# Check data types\n",
    "print(\"\\nClient data types:\")\n",
    "print(client_df.dtypes)\n",
    "print(\"\\nPrice data types:\")\n",
    "print(price_df.dtypes)\n",
    "\n",
    "print(\"\\n2. Data Exploration and Understanding\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Check for missing values and data types\n",
    "print(\"Client data info:\")\n",
    "print(client_df.info())\n",
    "print(\"\\nPrice data info:\")\n",
    "print(price_df.info())\n",
    "\n",
    "# Check unique values in key columns\n",
    "print(f\"\\nUnique client IDs: {client_df['id'].nunique()}\")\n",
    "print(f\"Total client records: {len(client_df)}\")\n",
    "print(f\"Unique price IDs: {price_df['id'].nunique()}\")\n",
    "print(f\"Total price records: {len(price_df)}\")\n",
    "\n",
    "# Check for churn column in client data\n",
    "print(f\"\\nChecking for target variable:\")\n",
    "if 'churn' in client_df.columns:\n",
    "    print(\"âœ“ Found 'churn' column in client data\")\n",
    "    print(f\"Churn distribution: {client_df['churn'].value_counts()}\")\n",
    "    print(f\"Churn rate: {client_df['churn'].mean():.3f}\")\n",
    "else:\n",
    "    print(\"âœ— No 'churn' column found in client data\")\n",
    "\n",
    "print(\"\\n3. Date Column Processing\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# Identify actual date columns that exist in the data\n",
    "date_columns_client = [col for col in client_df.columns if 'date' in col.lower()]\n",
    "date_columns_price = [col for col in price_df.columns if 'date' in col.lower()]\n",
    "\n",
    "print(f\"Date columns in client data: {date_columns_client}\")\n",
    "print(f\"Date columns in price data: {date_columns_price}\")\n",
    "\n",
    "def convert_to_epoch(date_series, date_format='%Y-%m-%d'):\n",
    "    \"\"\"\n",
    "    Convert date strings to normalized epoch time (0-1 scale)\n",
    "    \"\"\"\n",
    "    # Convert to datetime\n",
    "    dates = pd.to_datetime(date_series, format=date_format, errors='coerce')\n",
    "    \n",
    "    # Convert to epoch (seconds since 1970-01-01)\n",
    "    epoch_times = dates.astype('int64') // 10**9\n",
    "    \n",
    "    # Normalize to 0-1 scale\n",
    "    min_epoch = epoch_times.min()\n",
    "    max_epoch = epoch_times.max()\n",
    "    \n",
    "    if max_epoch == min_epoch:\n",
    "        return epoch_times * 0  # All same date\n",
    "    \n",
    "    normalized = (epoch_times - min_epoch) / (max_epoch - min_epoch)\n",
    "    \n",
    "    print(f\"Date range: {dates.min()} to {dates.max()}\")\n",
    "    print(f\"Epoch range: {min_epoch} to {max_epoch}\")\n",
    "    print(f\"Normalized range: {normalized.min():.3f} to {normalized.max():.3f}\")\n",
    "    \n",
    "    return normalized\n",
    "\n",
    "# Convert date columns in client data\n",
    "for col in date_columns_client:\n",
    "    if col in client_df.columns:\n",
    "        print(f\"\\nConverting {col}:\")\n",
    "        client_df[col] = convert_to_epoch(client_df[col])\n",
    "\n",
    "# Convert date columns in price data\n",
    "for col in date_columns_price:\n",
    "    if col in price_df.columns:\n",
    "        print(f\"\\nConverting {col}:\")\n",
    "        price_df[f'{col}_epoch'] = convert_to_epoch(price_df[col])\n",
    "\n",
    "print(\"\\n4. Merging Client and Price Data\")\n",
    "print(\"-\" * 35)\n",
    "\n",
    "# Merge on client ID\n",
    "print(\"Performing left join to keep all clients...\")\n",
    "merged_df = client_df.merge(price_df, on='id', how='left')\n",
    "print(f\"Merged dataset shape: {merged_df.shape}\")\n",
    "\n",
    "# Check for missing price data\n",
    "if len(date_columns_price) > 0:\n",
    "    missing_price = merged_df[date_columns_price[0]].isna().sum()\n",
    "    print(f\"Clients without price data: {missing_price}\")\n",
    "\n",
    "print(\"\\n5. Creating Price Statistical Features\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Identify price columns that actually exist\n",
    "price_columns = [col for col in price_df.columns if col.startswith('price_') and col != 'price_date']\n",
    "print(f\"Found price columns: {price_columns}\")\n",
    "\n",
    "if price_columns:\n",
    "    # Group price data by client ID to create statistical features\n",
    "    print(\"Calculating price statistics per client...\")\n",
    "    \n",
    "    # Create aggregation dictionary for existing price columns\n",
    "    agg_dict = {}\n",
    "    for col in price_columns:\n",
    "        agg_dict[col] = ['mean', 'std', 'min', 'max', 'last']\n",
    "    \n",
    "    price_stats = price_df.groupby('id').agg(agg_dict).round(6)\n",
    "    \n",
    "    # Flatten column names\n",
    "    price_stats.columns = ['_'.join(col).strip() for col in price_stats.columns]\n",
    "    price_stats = price_stats.reset_index()\n",
    "    \n",
    "    print(f\"Price statistics shape: {price_stats.shape}\")\n",
    "    print(\"Sample price statistics:\")\n",
    "    print(price_stats.head(2))\n",
    "    \n",
    "    # Merge price statistics with client data\n",
    "    final_df = client_df.merge(price_stats, on='id', how='left')\n",
    "    print(f\"Dataset with price features shape: {final_df.shape}\")\n",
    "else:\n",
    "    print(\"No price columns found - using client data only\")\n",
    "    final_df = client_df.copy()\n",
    "\n",
    "print(\"\\n6. One-Hot Encoding Categorical Variables\")\n",
    "print(\"-\" * 42)\n",
    "\n",
    "# Check for categorical columns that actually exist\n",
    "categorical_cols = final_df.select_dtypes(include=['object']).columns.tolist()\n",
    "print(f\"Found categorical columns: {categorical_cols}\")\n",
    "\n",
    "for col in categorical_cols:\n",
    "    if col != 'id':  # Don't encode ID column\n",
    "        print(f\"\\nUnique {col} values:\")\n",
    "        print(final_df[col].value_counts())\n",
    "        \n",
    "        # One-hot encode the column\n",
    "        dummies = pd.get_dummies(final_df[col], prefix=col)\n",
    "        print(f\"Created {len(dummies.columns)} dummy variables for {col}:\")\n",
    "        print(list(dummies.columns))\n",
    "        \n",
    "        # Add to dataset\n",
    "        final_df = pd.concat([final_df, dummies], axis=1)\n",
    "\n",
    "print(f\"\\nDataset shape after one-hot encoding: {final_df.shape}\")\n",
    "\n",
    "print(\"\\n7. Creating Consumption Features\")\n",
    "print(\"-\" * 35)\n",
    "\n",
    "# Check for consumption-related columns\n",
    "consumption_cols = [col for col in final_df.columns if 'cons' in col.lower()]\n",
    "print(f\"Found consumption columns: {consumption_cols}\")\n",
    "\n",
    "# Create consumption features if the required columns exist\n",
    "if 'cons_12m' in final_df.columns and 'forecast_cons_12m' in final_df.columns:\n",
    "    final_df['cons_pwr_12_mo_dif'] = final_df['cons_12m'] - final_df['forecast_cons_12m']\n",
    "    final_df['cons_pwr_12_mo_perc'] = np.where(\n",
    "        final_df['forecast_cons_12m'] != 0,\n",
    "        (final_df['cons_12m'] - final_df['forecast_cons_12m']) / final_df['forecast_cons_12m'],\n",
    "        0\n",
    "    )\n",
    "    print(\"Created consumption difference features\")\n",
    "else:\n",
    "    print(\"Required consumption columns not found - skipping consumption features\")\n",
    "\n",
    "print(\"\\n8. Creating Price Change Features\")\n",
    "print(\"-\" * 35)\n",
    "\n",
    "# Create price change features if price statistics exist\n",
    "if price_columns:\n",
    "    price_types = []\n",
    "    for col in price_columns:\n",
    "        price_type = col.replace('price_', '')\n",
    "        price_types.append(price_type)\n",
    "    \n",
    "    for price_type in price_types:\n",
    "        min_col = f'price_{price_type}_min'\n",
    "        max_col = f'price_{price_type}_max'\n",
    "        \n",
    "        if min_col in final_df.columns and max_col in final_df.columns:\n",
    "            # Price difference (max - min)\n",
    "            dif_col = f'price_{price_type}_dif'\n",
    "            final_df[dif_col] = final_df[max_col] - final_df[min_col]\n",
    "            \n",
    "            # Price percentage change\n",
    "            perc_col = f'price_{price_type}_perc'\n",
    "            final_df[perc_col] = np.where(\n",
    "                final_df[min_col] != 0,\n",
    "                (final_df[max_col] - final_df[min_col]) / final_df[min_col],\n",
    "                0\n",
    "            )\n",
    "            \n",
    "            print(f\"Created {dif_col} and {perc_col}\")\n",
    "else:\n",
    "    print(\"No price columns found - skipping price change features\")\n",
    "\n",
    "print(\"\\n9. Handle Target Variable\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# Handle churn column based on what actually exists\n",
    "if 'churn' in final_df.columns:\n",
    "    print(\"Using existing churn column\")\n",
    "    # Ensure churn is integer (0/1)\n",
    "    final_df['churn'] = final_df['churn'].astype(int)\n",
    "    \n",
    "    churn_distribution = final_df['churn'].value_counts()\n",
    "    print(\"Churn distribution:\")\n",
    "    print(churn_distribution)\n",
    "    print(f\"Churn rate: {final_df['churn'].mean():.3f}\")\n",
    "else:\n",
    "    print(\"No churn column found - creating from available data\")\n",
    "    # Try to create from date_end if it exists\n",
    "    if 'date_end' in final_df.columns:\n",
    "        # Before normalization, check if date_end indicates churn\n",
    "        original_client = pd.read_csv('SOURCE_client_data.csv')\n",
    "        if 'date_end' in original_client.columns:\n",
    "            final_df['churn'] = original_client['date_end'].notnull().astype(int)\n",
    "            print(\"Created churn from date_end column\")\n",
    "        else:\n",
    "            final_df['churn'] = 0  # Default to no churn\n",
    "            print(\"Created default churn column (all 0)\")\n",
    "    else:\n",
    "        final_df['churn'] = 0  # Default to no churn\n",
    "        print(\"Created default churn column (all 0)\")\n",
    "\n",
    "print(\"\\n10. Data Cleaning and Normalization\")\n",
    "print(\"-\" * 35)\n",
    "\n",
    "def clean_and_normalize_features(df):\n",
    "    \"\"\"\n",
    "    Clean and normalize numerical features for ML readiness\n",
    "    \"\"\"\n",
    "    # Replace infinite values with NaN\n",
    "    df = df.replace([np.inf, -np.inf], np.nan)\n",
    "    \n",
    "    # Get numerical columns (excluding ID, target, and categorical)\n",
    "    numerical_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    \n",
    "    # Remove columns that shouldn't be normalized\n",
    "    columns_to_exclude = ['id', 'churn']\n",
    "    numerical_cols = [col for col in numerical_cols if col not in columns_to_exclude]\n",
    "    \n",
    "    print(f\"Normalizing {len(numerical_cols)} numerical features...\")\n",
    "    \n",
    "    # Normalize numerical features to 0-1 scale\n",
    "    for col in numerical_cols:\n",
    "        if col in df.columns:\n",
    "            col_min = df[col].min()\n",
    "            col_max = df[col].max()\n",
    "            \n",
    "            if pd.notna(col_min) and pd.notna(col_max) and col_max != col_min:\n",
    "                df[col] = (df[col] - col_min) / (col_max - col_min)\n",
    "            else:\n",
    "                df[col] = df[col].fillna(0)\n",
    "    \n",
    "    # Fill any remaining NaN values with 0 (except for churn column)\n",
    "    for col in df.columns:\n",
    "        if col != 'churn' and df[col].isnull().any():\n",
    "            df[col] = df[col].fillna(0)\n",
    "    \n",
    "    print(\"Completed normalization and missing value handling\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "final_df = clean_and_normalize_features(final_df)\n",
    "\n",
    "print(\"\\n11. Remove Non-Numeric Columns\")\n",
    "print(\"-\" * 35)\n",
    "\n",
    "# Identify non-numeric columns\n",
    "non_numeric_cols = []\n",
    "for col in final_df.columns:\n",
    "    if col not in ['id', 'churn']:  # Keep ID and target for now\n",
    "        dtype = final_df[col].dtype\n",
    "        if dtype == 'object' or dtype == 'category':\n",
    "            non_numeric_cols.append(col)\n",
    "        elif dtype == 'bool':\n",
    "            print(f\"Converting boolean column '{col}' to int\")\n",
    "            final_df[col] = final_df[col].astype(int)\n",
    "\n",
    "print(f\"Found {len(non_numeric_cols)} non-numeric columns to remove:\")\n",
    "for col in non_numeric_cols:\n",
    "    print(f\"- {col}: {final_df[col].dtype}\")\n",
    "\n",
    "# Drop non-numeric columns\n",
    "if non_numeric_cols:\n",
    "    final_df = final_df.drop(columns=non_numeric_cols)\n",
    "    print(f\"Dropped {len(non_numeric_cols)} non-numeric columns\")\n",
    "\n",
    "print(\"\\n12. Final Dataset Preparation\")\n",
    "print(\"-\" * 32)\n",
    "\n",
    "# Remove ID and any remaining unnecessary columns\n",
    "columns_to_remove = ['id']\n",
    "# Add any date columns that were kept\n",
    "columns_to_remove.extend([col for col in final_df.columns if 'date' in col.lower() and col != 'churn'])\n",
    "\n",
    "final_df = final_df.drop(columns=[col for col in columns_to_remove if col in final_df.columns])\n",
    "\n",
    "# Reorder columns - features first, then target\n",
    "target_col = 'churn'\n",
    "feature_cols = [col for col in final_df.columns if col != target_col]\n",
    "final_df = final_df[feature_cols + [target_col]]\n",
    "\n",
    "print(f\"Final dataset shape: {final_df.shape}\")\n",
    "print(f\"Features: {len(feature_cols)}\")\n",
    "print(f\"Target variable: {target_col}\")\n",
    "\n",
    "print(\"\\n13. Data Quality Checks\")\n",
    "print(\"-\" * 25)\n",
    "\n",
    "# Check for any remaining issues\n",
    "print(\"Data quality checks:\")\n",
    "print(f\"- Missing values: {final_df.isnull().sum().sum()}\")\n",
    "print(f\"- Infinite values: {np.isinf(final_df.select_dtypes(include=[np.number])).sum().sum()}\")\n",
    "print(f\"- Duplicate rows: {final_df.duplicated().sum()}\")\n",
    "print(f\"- Data types: {final_df.dtypes.value_counts().to_dict()}\")\n",
    "\n",
    "# Verify all columns are numeric\n",
    "object_cols = final_df.select_dtypes(include=['object']).columns.tolist()\n",
    "if object_cols:\n",
    "    print(f\"WARNING: Found remaining object columns: {object_cols}\")\n",
    "else:\n",
    "    print(\"âœ“ All columns are now numeric\")\n",
    "\n",
    "print(\"\\n14. Saving Cleaned Dataset\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# Save the final dataset\n",
    "output_file = 'DATA_v2_churn.csv'\n",
    "final_df.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"Dataset saved as: {output_file}\")\n",
    "print(f\"Final shape: {final_df.shape}\")\n",
    "\n",
    "print(\"\\n15. Create Sample Dataset\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# Create a sample with 150 records\n",
    "sample_df = final_df.sample(n=min(150, len(final_df)), random_state=42)\n",
    "sample_file = 'SAMPLE_v2_churn.csv'\n",
    "sample_df.to_csv(sample_file, index=False)\n",
    "\n",
    "print(f\"Sample dataset saved as: {sample_file}\")\n",
    "print(f\"Sample shape: {sample_df.shape}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"DATA CLEANING AND MERGING COMPLETE!\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Display final summary\n",
    "print(\"\\nFinal column names:\")\n",
    "print(list(final_df.columns))\n",
    "\n",
    "print(\"\\nFirst 3 rows of cleaned dataset:\")\n",
    "print(final_df.head(10))\n",
    "\n",
    "print(f\"\\nDataset ready for machine learning!\")\n",
    "print(f\"Full dataset: {output_file}\")\n",
    "print(f\"Sample dataset: {sample_file}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
